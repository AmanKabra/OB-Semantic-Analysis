{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OB Semantic Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNq5nDd9PGzU8ge6Srovu3b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmanKabra/OB-Semantic-Analysis/blob/master/OB_Semantic_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fpxsG-0NcIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "472bbceb-3245-412f-f503-86201f0cc341"
      },
      "source": [
        "!pip3 install -qq twint\n",
        "!pip install -qq whatthelang\n",
        "!pip install nest_asyncio\n",
        "!pip install tweet-preprocessor\n",
        "!pip install --upgrade gensim\n",
        "!pip install transformers\n",
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.2MB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 16.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 225kB 16.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 19.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 19.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 18.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 7.8MB/s \n",
            "\u001b[?25h  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 788kB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 17.0MB/s \n",
            "\u001b[?25h  Building wheel for whatthelang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cysignals (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyfasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nest_asyncio\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/44/f2983c5be9803b08f89380229997e92c4bdd7a4a510ccee565b599d1bdc8/nest_asyncio-1.4.0-py3-none-any.whl\n",
            "Installing collected packages: nest-asyncio\n",
            "Successfully installed nest-asyncio-1.4.0\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 127kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.14.37)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.37)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 15.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 22.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 32.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=f94b99bb8935e88ee363ccabfa0b6b9e799dce7a31dfd99baba69c4a33e29a70\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgeJ0kGWNmjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c92fbff9-8a38-4c1c-85f9-c193171c05b8"
      },
      "source": [
        "# Import Library\n",
        "import twint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nest_asyncio\n",
        "from multiprocessing import Process\n",
        "nest_asyncio.apply()\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import json\n",
        "import csv\n",
        "import re #regular expression\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import preprocessor.api as p\n",
        "from preprocessor.api import clean, tokenize, parse\n",
        "import seaborn as sns   # for charts\n",
        "sns.set_style(\"whitegrid\");   # chart background style\n",
        "plt.rcParams['figure.dpi'] = 360   # for high res chart output\n",
        "import spacy   # for tokenising text\n",
        "from spacy.lang.en import English  # for tokenising text\n",
        "nlp = English()   # for tokenising text\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer\n",
        "import contractions\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo8eMkRGOIXL",
        "colab_type": "text"
      },
      "source": [
        "Scrape: The telegraph ,The times, The Economist, The Guardian, BBC news UK, The Independent, and The Financial Times.\n",
        "\n",
        "Handles: ['Telegraph', 'thetimes','TheEconomist','guardiannews','BBCNews','Independent','FinancialTimes']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDMElt6bLqUh",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvr9ZDrqLt97",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing and EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcA74WAzffiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Telegraph = pd.DataFrame()\n",
        "thetimes = pd.DataFrame()\n",
        "TheEconomist = pd.DataFrame()\n",
        "guardiannews = pd.DataFrame()\n",
        "BBCNews = pd.DataFrame()\n",
        "Independent = pd.DataFrame()\n",
        "FinancialTimes = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFncGxYKffWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = twint.Config()\n",
        "c.Store_object = True\n",
        "c.Pandas = True\n",
        "c.Username = \"FinancialTimes\"\n",
        "c.Since = \"2020-01-01\"\n",
        "c.Until = \"2020-08-01\"\n",
        "twint.run.Search(c)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1jrQJgtrRyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " FinancialTimes = FinancialTimes.append(twint.storage.panda.Tweets_df, ignore_index = True)\n",
        " FinancialTimes.to_csv('FinancialTimes.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0eop0WP8XWYW",
        "colab": {}
      },
      "source": [
        " Independent = Independent.append(twint.storage.panda.Tweets_df, ignore_index = True)\n",
        " Independent.to_csv('Independent.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1_zMdHx_8o3Q",
        "colab": {}
      },
      "source": [
        " BBCNews = BBCNews.append(twint.storage.panda.Tweets_df, ignore_index = True)\n",
        " BBCNews.to_csv('BBCNews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JwX0-oqW5zIc",
        "colab": {}
      },
      "source": [
        " guardiannews = guardiannews.append(twint.storage.panda.Tweets_df, ignore_index = True)\n",
        " guardiannews.to_csv('GuardianNews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zljOasJCzEYe",
        "colab": {}
      },
      "source": [
        " TheEconomist = TheEconomist.append(twint.storage.panda.Tweets_df, ignore_index = True)\n",
        " TheEconomist.to_csv('TheEconomist.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-T7tTB20uJJ_",
        "colab": {}
      },
      "source": [
        " thetimes = thetimes.append(twint.storage.panda.Tweets_df, ignore_index = True)\n",
        " thetimes.to_csv('TheTimes.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B9gWcsgf84q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " Telegraph = Telegraph.append(twint.storage.panda.Tweets_df, ignore_index = True)\n",
        " Telegraph.to_csv('Telegraph.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUftaWDnRauO",
        "colab_type": "text"
      },
      "source": [
        "Scraping done.\n",
        "\n",
        "We now proceed toward NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4yNr3EdRgsf",
        "colab_type": "text"
      },
      "source": [
        "Step 1: Data cleaning and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PPzPauutEuU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "890ea189-d2e6-4804-f4a8-036ec573cbdf"
      },
      "source": [
        "BBCNews = pd.read_csv(\"BBCNews.csv\")\n",
        "FinancialTimes = pd.read_csv(\"FinancialTimes.csv\")\n",
        "Independent = pd.read_csv(\"Independent.csv\")\n",
        "Telegraph = pd.read_csv(\"Telegraph.csv\")\n",
        "TheEconomist = pd.read_csv(\"TheEconomist.csv\")\n",
        "guardiannews = pd.read_csv(\"guardiannews.csv\")\n",
        "thetimes = pd.read_csv(\"thetimes.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-66d4d26cf910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFinancialTimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FinancialTimes.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mIndependent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Independent.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mTelegraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Telegraph.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mTheEconomist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TheEconomist.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mguardiannews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"guardiannews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 12365"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8fX1sh6s-Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fulldf = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4fORg_uRjyv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "493eed52-0844-4755-8080-842f2a779bdb"
      },
      "source": [
        "fulldf = BBCNews\n",
        "fulldf = fulldf.append(FinancialTimes, ignore_index = True)\n",
        "fulldf = fulldf.append(Independent, ignore_index = True)\n",
        "fulldf = fulldf.append(Telegraph, ignore_index = True)\n",
        "fulldf = fulldf.append(TheEconomist, ignore_index = True)\n",
        "fulldf = fulldf.append(guardiannews, ignore_index = True)\n",
        "fulldf = fulldf.append(thetimes, ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c10bca38f7f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfulldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBBCNews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfulldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfulldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFinancialTimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfulldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfulldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndependent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfulldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfulldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTelegraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfulldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfulldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTheEconomist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BBCNews' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGpCbp4iuHRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = fulldf[['tweet', 'hashtags','username','nlikes','nreplies','nretweets']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4korTRw4-twZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Total number of tweets: {len(df)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1APNcd7xROT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's find out how many tweets contain a hashtag\n",
        "tweets_with_hashtags = df.loc[df[\"tweet\"].str.contains(\"#\")]\n",
        "\n",
        "# view the number of tweets that contain a hashtag\n",
        "print(f\"Number of tweets containing hashtags: {len(tweets_with_hashtags)}\")\n",
        "\n",
        "# view the tweets that contain a hashtag\n",
        "tweets_with_hashtags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hj0sBi_2YRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# how many tweets contain a URL i.e. \"http\"?\n",
        "tweets_with_URLs = df.loc[df[\"tweet\"].str.contains(\"http\")]\n",
        "\n",
        "# view the number of tweets that contain a URL\n",
        "print(f\"Number of tweets containing URLs: {len(tweets_with_URLs)}\")\n",
        "\n",
        "# view the tweets that contain a URL\n",
        "tweets_with_URLs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaR32oL5219e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a single string containing all the tweets, as this will be needed to be able to create a wordcloud\n",
        "tweet_string = \" \".join(tweet for tweet in df[\"tweet\"])\n",
        "\n",
        "# replace all the mentions (e.g. @Anurag_Gupta) from the tweets with '@USER'\n",
        "tweet_string = re.sub(r'@\\w+','@USER ', tweet_string)\n",
        "\n",
        "# replace all the URLs with '_URL_'\n",
        "tweet_string = re.sub(r'http\\S+','_URL_ ', tweet_string)\n",
        "\n",
        "# convert the text to lower case so, for example, instead of having \"Be\" and \"be\" included\n",
        "# as 2 separate words, we'd only have \"be\"\n",
        "tweet_string = tweet_string.lower()\n",
        "\n",
        "# remove extra white spaces so there is only one space between words\n",
        "tweet_string = re.sub(r'\\s+',' ', tweet_string)\n",
        "\n",
        "# view the first 200 elements of the string to check this worked as expected\n",
        "tweet_string[0:200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZDkmICK3iFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the wordcloud\n",
        "tweet_wordcloud = WordCloud(background_color=\"white\", \n",
        "                              max_words=100, \n",
        "                             ).generate(tweet_string)\n",
        "\n",
        "# view the wordcloud\n",
        "plt.imshow(tweet_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTaY82vHAe3N",
        "colab_type": "text"
      },
      "source": [
        "Trying another alternative (by putting empty space in place of URLs and Tags)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKL93p2jAeO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a single string containing all the tweets, as this will be needed to be able to create a wordcloud\n",
        "tweet_string = \" \".join(tweet for tweet in df[\"tweet\"])\n",
        "\n",
        "# replace all the mentions (e.g. @Anurag_Gupta) from the tweets with '@USER'\n",
        "tweet_string = re.sub(r'@\\w+',' ', tweet_string)\n",
        "\n",
        "# replace all the URLs with '_URL_'\n",
        "tweet_string = re.sub(r'http\\S+',' ', tweet_string)\n",
        "\n",
        "# convert the text to lower case so, for example, instead of having \"Be\" and \"be\" included\n",
        "# as 2 separate words, we'd only have \"be\"\n",
        "tweet_string = tweet_string.lower()\n",
        "\n",
        "# remove extra white spaces so there is only one space between words\n",
        "tweet_string = re.sub(r'\\s+',' ', tweet_string)\n",
        "\n",
        "# view the first 200 elements of the string to check this worked as expected\n",
        "tweet_string[0:200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czsc0qLLAnIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the wordcloud\n",
        "tweet_wordcloud = WordCloud(background_color=\"white\", \n",
        "                              max_words=100, \n",
        "                             ).generate(tweet_string)\n",
        "\n",
        "# view the wordcloud\n",
        "plt.imshow(tweet_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYVUHJXbAsWU",
        "colab_type": "text"
      },
      "source": [
        "Running NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb_OL5hV5Jus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a spacy document by pointing spacy to the tweet string\n",
        "tweet_doc = nlp(tweet_string[0:1000000])\n",
        "\n",
        "# get all tokens that aren't punctuation\n",
        "tweet_words = [token.text for token in tweet_doc if token.is_punct != True]\n",
        "\n",
        "# get the frequency of each word (token) in the tweet string\n",
        "tweet_word_freq = Counter(tweet_words)\n",
        "\n",
        "# get the 5 most frequent words\n",
        "five_most_common_words = tweet_word_freq.most_common(5)\n",
        "\n",
        "# view the 5 most common words\n",
        "five_most_common_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EisUIYo7MmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get all tokens that aren't punctuation and aren't stopwords\n",
        "tweet_words = [token.text for token in tweet_doc if token.is_punct != True and \\\n",
        "token.is_stop != True]\n",
        "\n",
        "# get the frequency of each word (token) in the tweet string\n",
        "tweet_word_freq = Counter(tweet_words)\n",
        "\n",
        "# re-create the Pandas dataframe containing the \n",
        "# tokens (words) and their frequencies\n",
        "freq_df = pd.DataFrame.from_dict(tweet_word_freq, orient='index').reset_index()\n",
        "\n",
        "# rename the columns to \"word\" and \"freq\"\n",
        "freq_df.columns=[\"word\", \"freq\"]\n",
        "\n",
        "# display a bar chart showing the top 25 words and their\n",
        "# frequencies (which will exclude the stopwords this time)\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "sns.barplot(data=freq_df.sort_values(by=\"freq\", ascending=False).head(25), \n",
        "            y=\"word\", \n",
        "            x=\"freq\", \n",
        "            color='#7bbcd5')\n",
        "plt.ylabel(\"Word\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.title(\"Top 25 Most Frequent Words (Excluding Stopwords)\")\n",
        "plt.xticks([0,1,2,3])\n",
        "sns.despine();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5V5bNm8B1o4",
        "colab_type": "text"
      },
      "source": [
        "Word cloud without stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OzUf7HiBzNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get all tokens that aren't punctuation and aren't stopwords\n",
        "tweet_words = [token.text for token in tweet_doc if token.is_punct != True and \\\n",
        "token.is_stop != True]\n",
        "\n",
        "# get the frequency of each word (token) in the tweet string\n",
        "tweet_word_freq = Counter(tweet_words)\n",
        "\n",
        "# re-create the Pandas dataframe containing the \n",
        "# tokens (words) and their frequencies\n",
        "freq_df = pd.DataFrame.from_dict(tweet_word_freq, orient='index').reset_index()\n",
        "\n",
        "# rename the columns to \"word\" and \"freq\"\n",
        "freq_df.columns=[\"word\", \"freq\"]\n",
        "\n",
        "freq_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-au9w3zCid2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listToStr = ' '.join([str(elem) for elem in tweet_words]) \n",
        "\n",
        "# create the wordcloud\n",
        "tweet_wordcloud = WordCloud(background_color=\"white\", \n",
        "                              max_words=100, \n",
        "                             ).generate(listToStr)\n",
        "\n",
        "# view the wordcloud\n",
        "plt.imshow(tweet_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO-goRLCCrWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_content(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text) #remove urls\n",
        "    text=re.sub(r'\\S+\\.com\\S+','',text) #remove urls\n",
        "    text=re.sub(r'\\@\\w+','',text) #remove mentions\n",
        "    text =re.sub(r'\\#\\w+','',text) #remove hashtags\n",
        "    return text\n",
        "def process_text(text, stem=False): #clean text\n",
        "    text=remove_content(text)\n",
        "    text = re.sub('[^A-Za-z]', ' ', text.lower()) #remove non-alphabets\n",
        "    tokenized_text = word_tokenize(text) #tokenize\n",
        "    clean_text = [\n",
        "         word for word in tokenized_text\n",
        "         if word not in stop_words\n",
        "    ]\n",
        "    if stem:\n",
        "        clean_text=[wordnet_lemmatizer.lemmatize(word) for word in clean_text]\n",
        "    return ' '.join(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsvM9Fy508IZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "24770a85-dd7d-41b4-c610-8925a77bd2c4"
      },
      "source": [
        "df['cleaned_tweets']=df['tweet'].apply(lambda x: process_text(x, True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULnKT7S93smc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp=' '.join(df['cleaned_tweets'].tolist())\n",
        "wordcloud = WordCloud(width = 800, height = 500, \n",
        "                background_color ='white', \n",
        "                min_font_size = 10).generate(temp)\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cPSwEJD9fi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob\n",
        "df['sentiment']=df['tweet'].apply(lambda x:TextBlob(x).sentiment[0])\n",
        "df['subject']=df['tweet'].apply(lambda x: TextBlob(x).sentiment[1])\n",
        "df['polarity']=df['sentiment'].apply(lambda x: 'pos' if x>=0 else 'neg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y-FcqYD-nBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "c = collections.Counter(df['polarity'])\n",
        "c = sorted(c.items())\n",
        "polarity = [i[0] for i in c]\n",
        "freq = [i[1] for i in c]\n",
        "\n",
        "f, ax = plt.subplots()\n",
        "\n",
        "plt.bar(polarity, freq)\n",
        "plt.title(\"Polarity of tweets\")\n",
        "plt.xlabel(\"Polarity\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnbqVxHuwFiL",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIHthHSsBSU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coviddf = df[df['tweet'].str.contains(\"COVID|Corona|Coronavirus|SARS-CoV-2|COVID-19|Ncov|Vaccination|Vaccine|Quarantine|Pandemic|Social Distancing|Distancing|NHS|Doctor|Outbreak|PPE|Sanitiser|Wuhan|Death|Lockdown|Bat|Virus|Mask|Respirator|Lung|China|Infect|Isolation|Ventilator|Test kits|Recession|Testkit|Disease|Transmission|Transmit|AstraZeneca|GeneXpert|RT-PCR|RTPCR|Oxford|Community|Spread|Herd|Comorbid|Flatten|Curve|Immunity|Antibodies|Antibody|Mortality|Treatment|Preventive|Prevention|Symptom|Throat|Cure|Die|Sick|World Health|Fomite|Tracing|Trace|Patient|Contagious|Gather|Restrict|Novel|Countrywide|Incubation|Drug|Hydroxychloroquine|Essential|Crisis|Containment|Contactless|Epidemiology|Thermometer|Surveillance|Immunosuppress|Incubation|Index case|Index patient|National emergency|Intensivist|Phlebotomist|Patient zero|Remdesivir|PUI|Respirator|Shelter-in-place|Spanish flu|Super-spreader|Pneumonia|Kung flu|Faceshield|Restaurants closed|Bars closed\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TyHVnFQHl3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "urlarray = [] \n",
        "for line in df[\"tweet\"]:\n",
        "  url = re.findall('(?P<url>https?://[^\\s]+)', line)\n",
        "  urlarray.append(url)\n",
        "\n",
        "df[\"URL1\"] = urlarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc-1Q6NiJFaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "urlarray = [] \n",
        "for line in df[\"tweet\"]:\n",
        "  url = re.findall('((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)', line)\n",
        "  urlarray.append(url)\n",
        "df[\"URL2\"] = urlarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g89rHmv3KH6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['URL1'].equals(df['URL2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th2iwgNgLBZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(df[df['URL1'] != df['URL2']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARTn_mG-tklG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "for i in sent_tokenize(df['cleaned_tweets']): \n",
        "    temp = []      \n",
        "    # tokenize the sentence into words \n",
        "    for j in word_tokenize(i): \n",
        "        temp.append(j.lower()) \n",
        "    data.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB2vqN-4tkqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt = TweetTokenizer()\n",
        "b=[]\n",
        "for t in df[\"tweet\"]:\n",
        "  a = tt.tokenize(t)\n",
        "  b.append(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dENroctutkbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "model = Word2Vec(b, min_count=1)\n",
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "# save model\n",
        "model.save('model.bin')\n",
        "# load model\n",
        "new_model = Word2Vec.load('model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0tWeYBbynl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79165998-1467-4086-eb1c-8f21ff7f6363"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7f6ed463a7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_Cr8hHJjCe",
        "colab_type": "text"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufKFxU7MJn5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bd0e731-7b7d-4bb5-ddf3-a3b9682cb5d5"
      },
      "source": [
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LUfuUnzJzM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dcfd792f-577c-4954-b908-93f3afd2342b"
      },
      "source": [
        "# If there's a GPU available\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r54VHZdMReu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "aa7c6a93-8c13-4745-fac3-173d14710d67"
      },
      "source": [
        "BBCNews = pd.read_csv(\"BBCNews.csv\")\n",
        "FinancialTimes = pd.read_csv(\"FinancialTimes.csv\")\n",
        "Independent = pd.read_csv(\"Independent.csv\")\n",
        "Telegraph = pd.read_csv(\"Telegraph.csv\")\n",
        "TheEconomist = pd.read_csv(\"TheEconomist.csv\")\n",
        "guardiannews = pd.read_csv(\"guardiannews.csv\")\n",
        "thetimes = pd.read_csv(\"thetimes.csv\")\n",
        "\n",
        "fulldf = pd.DataFrame()\n",
        "\n",
        "fulldf = BBCNews\n",
        "fulldf = fulldf.append(FinancialTimes, ignore_index = True)\n",
        "fulldf = fulldf.append(Independent, ignore_index = True)\n",
        "fulldf = fulldf.append(Telegraph, ignore_index = True)\n",
        "fulldf = fulldf.append(TheEconomist, ignore_index = True)\n",
        "fulldf = fulldf.append(guardiannews, ignore_index = True)\n",
        "fulldf = fulldf.append(thetimes, ignore_index = True)\n",
        "\n",
        "df = fulldf[['tweet', 'hashtags','date','username','nlikes','nreplies','nretweets']]\n",
        "\n",
        "cdf = df[df['tweet'].str.contains(\"COVID|Corona|Coronavirus|SARS-CoV-2|COVID-19|Ncov|Vaccination|Vaccine|Quarantine|Pandemic|Social Distancing|Distancing|NHS|Doctor|Outbreak|PPE|Sanitiser|Wuhan|Death|Lockdown|Bat|Virus|Mask|Respirator|Lung|China|Infect|Isolation|Ventilator|Test kits|Recession|Testkit|Disease|Transmission|Transmit|AstraZeneca|GeneXpert|RT-PCR|RTPCR|Oxford|Community|Spread|Herd|Comorbid|Flatten|Curve|Immunity|Antibodies|Antibody|Mortality|Treatment|Preventive|Prevention|Symptom|Throat|Cure|Die|Sick|World Health|Fomite|Tracing|Trace|Patient|Contagious|Gather|Restrict|Novel|Countrywide|Incubation|Drug|Hydroxychloroquine|Essential|Crisis|Containment|Contactless|Epidemiology|Thermometer|Surveillance|Immunosuppress|Incubation|Index case|Index patient|National emergency|Intensivist|Phlebotomist|Patient zero|Remdesivir|PUI|Respirator|Shelter-in-place|Spanish flu|Super-spreader|Pneumonia|Kung flu|Faceshield|Restaurants closed|Bars closed\")]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dM__qxrH6kJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "04557398-d8d6-4a34-adf8-31171d63bb6c"
      },
      "source": [
        "#creating URL column\n",
        "\n",
        "urlarray = [] \n",
        "for line in cdf[\"tweet\"]:\n",
        "  url = re.findall('((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)', line)\n",
        "  urlarray.append(url)\n",
        "cdf[\"URL\"] = urlarray"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6lOSW7UNng_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "33217de7-5799-4caf-c98c-0834dde01ef6"
      },
      "source": [
        "#creating fuction to remove unwanted parts from COVID tweets\n",
        "\n",
        "def remove_content(text):\n",
        "  text = re.sub(r\"http\\S+\", \"\", text) #remove urls\n",
        "  text=re.sub(r'\\S+\\.com\\S+','',text) #remove urls\n",
        "  text=re.sub(r'\\@\\w+','',text) #remove mentions\n",
        "  text =re.sub(r'\\#\\w+','',text) #remove hashtags\n",
        "  text = contractions.fix(text) #replace contractions with full forms (eg. don't becomes do not)\n",
        "  text = re.sub('[^A-Za-z]', ' ', text.lower()) #remove non-alphabets\n",
        "  text = text.lower() #converts to lower case\n",
        "  text = re.sub(r'\\s+',' ', text) #remove extra white space\n",
        "  return text\n",
        "\n",
        "#calling the function\n",
        "cdf['cleaned_tweets']=cdf['tweet'].apply(lambda x: remove_content(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCRWEbN9GD3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " cdf.to_csv('cdf.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5nFKO1FMkiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = cdf.cleaned_tweets.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pfwNcOUM5Z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqlWK5zHNVtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfWBya2bOUK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "  # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "  # Update the maximum sentence length.\n",
        "  max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "print('This is not as per character length, but as per \"token\" length according to BERT token creations')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6PatE9VOuR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}